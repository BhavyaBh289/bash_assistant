# -*- coding: utf-8 -*-
"""Copy of Welcome to Colaboratory

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lYSEjYYK0cS6j-2EI0fsbi636K5ZioJM
"""

# pip install llama-cpp-python==0.1.9
#
# from google.colab import drive
# drive.mount('/content/drive')

from transformers import AutoModelForCausalLM, AutoTokenizer
from huggingface_hub import login

# Replace "YOUR_API_TOKEN" with your actual Hugging Face API token
login("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")

# Specify the model name
model_name = "bigcode/starcoderbase-1b"

# Load the model and tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# Save the model and tokenizer locally
local_dir = "./starcoderbase-local"
tokenizer.save_pretrained(local_dir)
model.save_pretrained(local_dir)

print(f"Model and tokenizer saved to {local_dir}")

# from llama_cpp import Llama
# import json
# import torch
# from transformers import AutoTokenizer, AutoModelForCausalLM, AdamW
# from peft import get_peft_model, LoraConfig, TaskType
# from torch.utils.data import DataLoader, Dataset
#
# model_name = "./starcoderbase-local"
# batch_size = 2  # Adjust based on memory constraints
# num_epochs = 3
#
# # Custom Dataset class to load data in batches
# class CommandDataset(Dataset):
#     def __init__(self, data, tokenizer, max_length=512):
#         self.data = data
#         self.tokenizer = tokenizer
#         self.max_length = max_length
#
#     def __len__(self):
#         return len(self.data)
#
#     def __getitem__(self, idx):
#         item = self.data[idx]
#         inputs = self.tokenizer(item['nl_command'], return_tensors="pt", padding="max_length", truncation=True, max_length=self.max_length)
#         outputs = self.tokenizer(item['bash_code'], return_tensors="pt", padding="max_length", truncation=True, max_length=self.max_length)
#
#         # Mask padding tokens in labels
#         labels = outputs['input_ids']
#         labels[labels == self.tokenizer.pad_token_id] = -100
#         return inputs['input_ids'].squeeze(0), labels.squeeze(0)
#
# # Load model and tokenizer
# model = AutoModelForCausalLM.from_pretrained(model_name).to("cuda" if torch.cuda.is_available() else "cpu")
# model = model.half()  # Convert model to half precision for memory savings
# tokenizer = AutoTokenizer.from_pretrained(model_name)
#
# if tokenizer.pad_token is None:
#     tokenizer.pad_token = tokenizer.eos_token
#
# # Load and prepare data
# with open('/content/drive/MyDrive/majorproj/data_train.json', 'r') as file:
#     data = json.load(file)
#
# dataset = CommandDataset(data, tokenizer)
# dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)
#
# # Define LoRA configuration and integrate with the model
# lora_config = LoraConfig(
#     task_type=TaskType.CAUSAL_LM,
#     r=16,
#     lora_alpha=32,
#     lora_dropout=0.1,
#     target_modules=["attn.c_attn", "attn.c_proj", "mlp.c_fc", "mlp.c_proj"]
# )
# model = get_peft_model(model, lora_config)
#
# # Optimizer
# optimizer = AdamW(model.parameters(), lr=5e-5)
#
# # Training loop with gradient accumulation
# accumulation_steps = 4  # Accumulate gradients for multiple steps to simulate larger batches
#
# model.train()
# for epoch in range(num_epochs):
#     epoch_loss = 0
#     for step, (batch_inputs, batch_labels) in enumerate(dataloader):
#         batch_inputs, batch_labels = batch_inputs.to(model.device), batch_labels.to(model.device)
#         outputs = model(input_ids=batch_inputs, labels=batch_labels)
#         loss = outputs.loss / accumulation_steps
#         loss.backward()
#
#         # Only step optimizer after accumulating gradients
#         if (step + 1) % accumulation_steps == 0:
#             optimizer.step()
#             optimizer.zero_grad()
#
#         epoch_loss += loss.item() * accumulation_steps
#
#     print(f"Epoch {epoch+1}, Loss: {epoch_loss / len(dataloader)}")
#
# # Save the fine-tuned model
# tokenizer.save_pretrained('/content/drive/MyDrive/majorproj/fine-tuned-lora-model')
# model.save_pretrained('/content/drive/MyDrive/majorproj/fine-tuned-lora-model')
